# Predicting IMDB Movie Financial Success

## 1. Introduction

This data science project aims to predict the **Gross Revenue (Gross)** of films based on their characteristics, utilizing the **IMDB Movies Dataset** available on Kaggle.

The main objective is to build a model capable of estimating a film's commercial success **before its release**, by leveraging information such as:
- The names of the **actors** and the **director**
- The film's **genre**
- Its **runtime**
- Its **release year**
- Its **classification certificate**
- The **number of IMDB votes**, etc.

### Context
The dataset contains 1,000 of the highest-rated films on IMDB, with variables including:
- `Series_Title`, `Director`, `Stars`, `Genre`, `Runtime`, `Certificate`
- `IMDB_Rating`, `Meta_score`, `No_of_votes`, `Gross` (the target to predict)

This project was carried out using a **learning-by-doing approach**, applying all the key steps of a data science lifecycle:
1. Data exploration and cleaning
2. Feature preparation
3. Modeling and evaluation
4. Result interpretation

The final goal: **to understand the factors that truly influence a film's revenue** and to build a robust model for prediction.

## 2. Data Exploration (EDA)

Prior to any modeling, a step of data exploration and understanding was performed.

### Dataset Description

The dataset contains **1,000 films** with the following variables:

| Variable | Description |
|---|---|
| `Series_Title` | Film Title |
| `Released_Year` | Release Year |
| `Certificate` | Film Classification (e.g., PG-13, R, U) |
| `Runtime` | Film Duration (e.g., "142 min") |
| `Genre` | Film Genres (e.g., "Action, Drama, Sci-Fi") |
| `IMDB_Rating` | Average IMDB Rating |
| `Meta_score` | Metacritic Score |
| `Director` | Director |
| `Star1` to `Star4` | Lead Actors |
| `No_of_Votes` | Number of Votes received on IMDB |
| `Gross` | Gross Revenue generated by the film (in $) |

### Missing Data

- `Certificate`: 101 missing values
- `Meta_score`: 157 missing values
- `Gross`: 169 missing values (our **target variable**)

Decisions were made:
- Replace missing `Meta_score` values with its **mean**
- Replace missing `Certificate` values with its **most frequent value (mode)**
- Drop rows where `Gross` is missing (as it cannot be predicted)

### ðŸ“Š Exploratory Visualizations

Several visualizations were created to better understand the data:

- **`Gross` Distribution**: highly skewed, with a few extreme blockbusters
- **Correlation between variables**: `No_of_Votes` is the most correlated with `Gross` (0.57)
- **Most Profitable Genres**: `Adventure`, `Sci-Fi`, `Action` dominate
- **Most Bankable Directors**: Anthony Russo, James Cameron, J.J. Abrams...
- **Time Trend**: more recent films tend to earn more revenue

---

## 3. Data Preparation

### Categorical Feature Processing

Columns such as `Genre`, `Certificate`, `Director`, `Star1â€“4` were encoded:

- `Genre`, `Certificate`: **One-Hot Encoding**
- `Director`, `Stars`: **Target Encoding** based on the average revenue per person
  - The 4 `Star1â€“4` columns were combined and replaced by an average score `Stars_encoded`

### Target Variable Transformation

Since the revenue (`Gross`) has a highly skewed distribution, a **logarithmic transformation** was applied:

    df['log_Gross'] = np.log(df['Gross'] + 1)

This makes the distribution more normal and improves the learning process for regression models.

## Numerical Variable Standardization

- Numerical variables (No_of_Votes, Runtime, Stars_encoded, etc.) were standardized using StandardScaler
- The target variable `Gross` was not standardized, only log-transformed.

## Dataset Split

The dataset was separated into 3 parts:

- 70% Training (X_train, y_train)
- 15% Validation (X_val, y_val)
- 15% Final Test (X_test, y_test)

## 4. Modeling

The goal is to predict the gross revenue (`Gross`) of a film based on its characteristics.
Several regression models were tested, starting with a simple model and moving toward more powerful ones.

---

### Linear Regression (Baseline)

A first **Linear Regression** model was trained on the transformed variable `log(Gross)`.

#### Result:
The resulting performance was **extremely poor**:

- **MAE:** astronomical values
- **RÂ²:** strongly negative (worse than a model that predicts the mean)

 **Interpretation**: the relationship between the variables and revenue is **not linear**, making this model **unsuitable**.

---

### Random Forest Regressor

A **Random Forest** model, based on an ensemble of decision trees, was then used.

#### Advantages:
- Captures **nonlinear relationships**
- Handles **feature interactions** well
- Resistant to outliers
- No need for feature normalization

#### Results (Validation):
- **MAE:** $14.4 M
- **RMSE:** $34.8 M
- **RÂ²:** 0.8700

These results show that the model is **highly performant**, explaining over 87% of the revenue variance on films never seen during training.

---

### âš™ï¸ Optimization with GridSearchCV

Hyperparameter tuning (`n_estimators`, `max_depth`, etc.) was conducted to refine the model.

Result: The performance remained **nearly identical**, confirming that the initial model was already well-tuned.

---

### XGBoost Model Test

An **XGBoost** (Extreme Gradient Boosting) model was also tested.
This model is highly performant on many problems, but in this specific case, it slightly **underperformed** compared to Random Forest.

#### XGBoost Results (Validation):
- **MAE:** $15.5 M
- **RMSE:** $38.2 M
- **RÂ²:** 0.8438

> Conclusion: The **Random Forest** remains the **optimal model** for this project.

---

### Final Evaluation on the Test Set

The Random Forest model was then tested on **completely unseen data** (`X_test`, `y_test`), with the inverse logarithmic transformation applied to the predictions.

#### Final Test Results:
- **MAE:** $18.1 M
- **RMSE:** $40.0 M
- **RÂ²:** 0.8642

 This confirms that the model is **robust, reliable, and generalizable**.

## 5. Performance Evaluation and Feature Importance

### Metrics Used

Three metrics were used to evaluate model performance:

| Metric | Description |
|---|---|
| **MAE (Mean Absolute Error)** | Gives a direct idea of the average difference between the prediction and the true value |
| **RMSE (Root Mean Squared Error)** | Penalizes large errors more severely than MAE |
| **RÂ² (Coefficient of Determination)** | Measures the proportion of variance explained by the model (ideally between 0 and 1) |

---

### Final Performance Summary of the Random Forest Model

| Dataset | MAE (in $) | RMSE (in $) | RÂ² |
|---|---|---|---|
| **Validation** | 14,455,614 | 34,857,261 | 0.8700 |
| **Final Test** | 18,135,556 | 40,056,388 | 0.8642 |

> The model is capable of predicting film revenue with **less than 20 million average error**, which is excellent given the variability of films (some grossing a few million, others several hundred million).

---

### Feature Importance

The Random Forest model allows for the analysis of the **relative importance of each feature** in predicting revenue.

#### Top 10 Most Important Features:

1. **Stars_encoded** â†’ "Bankable" level of the actors
2. **Director_encoded** â†’ Director's reputation
3. **Released_Year** â†’ Trend of recent films to earn more
4. **Genre_Crime, Thriller**
5. **No_of_Votes** â†’ Popularity on IMDB
6. **Runtime** â†’ Length of the film
7. **Genre_Drama, History**
8. **Genre_Drama**
9. **Genre_Comedy, Drama, War**
10. **Certificate_Passed**

#### Interpretation:

- The **Stars_encoded** and **Director_encoded** features clearly dominate, showing that **casting and the director are the best predictors of success**.
- The **genre**, **number of votes**, and **runtime** have a more marginal influence.
- The **classification certificate** (e.g., PG-13, R...) seems less determining.

 These results are consistent with business intuition: **big names attract audiences**, and thus revenue.

## 6. Conclusion and Business Interpretation

### Objective Achieved

The objective of this project was to **predict the Gross Revenue of a film** using characteristics available **before its release**.
Thanks to rigorous data processing, adapted modeling, and thorough testing, a **high-performing and generalizable model** was built.

### Key Steps Summary

- **IMDB dataset exploration** to understand general trends
- **Data cleaning and transformation**, including the log transformation of the target
- **Intelligent encoding** of categorical features like `Stars`, `Director`, `Genre`
- **Testing of multiple models**, including Linear Regression (baseline), Random Forest (final), and XGBoost (comparative)
- **Hyperparameter optimization** with GridSearch
- **Final evaluation on unseen data** with robust metrics (RÂ² â‰ˆ 0.86)

### What the Model Teaches Us

The analysis of the most important features reveals **very clear insights**:

- The **lead actors** (`Stars_encoded`) and the **director** (`Director_encoded`) are **the two most predictive factors** of a film's commercial success.
- Features like **genre**, **number of IMDB votes**, or **runtime** have a **more limited impact**.
- The **classification certificate** (PG-13, R...) has a marginal influence.

 In other words, the **casting** and the **talent behind the camera** are the determining elements for maximizing a film's revenue.

### Perspectives and Limitations

#### Strengths:
- High-performing, explainable, and reproducible model
- Complete end-to-end pipeline
- Interpretable results thanks to importance analysis

#### Limitations:
- Data limited to 1,000 films â†’ not representative of the entire industry
- Certain features like `Overview` or budgets were not utilized
- The model does not account for marketing campaign effects, release date, etc.

### Possible Improvements

- Integrate NLP techniques on `Overview` (film synopsis)
- Use additional data (budget, studio, exact release date)
- Test other boosting algorithms (CatBoost, LightGBM)
- Deploy the model in an interactive application (API or dashboard)

This project demonstrates that it is possible to **predict a film's financial success with good precision** using simple features, **particularly those related to casting and the director**.
It is a concrete demonstration of the **predictive value of data** in an artistic and economic context like cinema.

> Thank you for reading.
